sqoop eval \
 --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
 --username retail_user \
 --password itversity \
 --query "SELECT * FROM orders WHERE order_status NOT LIKE '%COMPLETE%' LIMIT 10"

order_id | order_date | order_customer_id | order_status
68883

sqoop import \
 --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
 --username retail_user \
 --password itversity \
 --table orders \
 --delete-target-dir \
 --target-dir /user/oathbreaker89/ARUN4/problem1/input \
 --autoreset-to-one-mapper \
 --fields-terminated-by "\t"

hadoop fs -ls /user/oathbreaker89/ARUN4/problem1/input


#2
sqoop import \
 --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
 --username retail_user \
 --password itversity \
 --table orders \
 --delete-target-dir \
 --target-dir /user/oathbreaker89/ARUN4/problem2/input \
 --as-avrodatafile

hadoop fs -ls /user/oathbreaker89/ARUN4/problem2/input 

#3
sqoop import \
 --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
 --username retail_user \
 --password itversity \
 --table orders \
 --delete-target-dir \
 --target-dir /user/oathbreaker89/ARUN4/problem3/input \
 --as-parquetfile

hadoop fs -ls /user/oathbreaker89/ARUN4/problem3/input

#4
spark-shell \
--master yarn \
--packages com.databricks:spark-avro_2.10:2.0.1

import com.databricks.spark.avro._

var orderDF = sqlContext.read.avro("/user/oathbreaker89/ARUN4/problem2/input")
orderDF.printSchema()
orderDF.show()

sqlContext.setConf("spark.sql.parquet.compression.codec","Snappy")
orderDF.write.parquet("/user/oathbreaker89/ARUN4/problem4/parquet-snappy")

hadoop fs -ls /user/oathbreaker89/ARUN4/problem4/parquet-snappy

orderDF.rdd.map(x => x.mkString(",")).saveAsTextFile("/user/oathbreaker89/ARUN4/problem4/text-gzip",classOf[org.apache.hadoop.io.compress.GzipCodec])
hadoop fs -ls /user/oathbreaker89/ARUN4/problem4/text-gzip

orderDF.rdd.map(x => x.mkString("\t")).saveAsTextFile("/user/oathbreaker89/ARUN4/problem4/text-tab")
hadoop fs -ls /user/oathbreaker89/ARUN4/problem4/text-tab

orderDF.rdd.map(x => (x(0).toString,x(0)+"\t"+x(1)+"\t"+x(2)+"\t"+x(3))).saveAsSequenceFile("/user/oathbreaker89/ARUN4/problem4/sequence-file")
hadoop fs -ls /user/oathbreaker89/ARUN4/problem4/sequence-file

orderDF.rdd.map(x => x.mkString("\t")).saveAsTextFile("/user/oathbreaker89/ARUN4/problem4/text-snappy",classOf[org.apache.hadoop.io.compress.SnappyCodec])
hadoop fs -ls /user/oathbreaker89/ARUN4/problem4/text-snappy

#5
var orderDF_parquet = sqlContext.read.parquet("/user/oathbreaker89/ARUN4/problem3/input")
orderDF_parquet.show()

sqlContext.setConf("spark.sql.parquet.compression.codec","uncompressed")
orderDF_parquet.write.parquet("/user/oathbreaker89/ARUN4/problem4/parquet-uncompressed")
hadoop fs -ls /user/oathbreaker89/ARUN4/problem4/parquet-uncompressed

sqlContext.setConf("spark.sql.avro.compression.codec","Snappy")
orderDF_parquet.write.format("com.databricks.spark.avro").save("/user/oathbreaker89/ARUN4/problem5/avro-snappy")
hadoop fs -ls /user/oathbreaker89/ARUN4/problem5/avro-snappy

#6
var orderDF_avro = sqlContext.read.avro("/user/oathbreaker89/ARUN4/problem5/avro-snappy")

orderDF_avro.toJSON.saveAsTextFile("/user/oathbreaker89/ARUN4/problem5/json-uncompress")
hadoop fs -ls /user/oathbreaker89/ARUN4/problem5/json-uncompress

orderDF_avro.toJSON.saveAsTextFile("/user/oathbreaker89/ARUN4/problem5/json-gzip",classOf[org.apache.hadoop.io.compress.GzipCodec])
hadoop fs -ls /user/oathbreaker89/ARUN4/problem5/json-gzip


#7
var orderDF_json = sqlContext.read.json("/user/oathbreaker89/ARUN4/problem5/json-gzip")
orderDF_json.show()

orderDF_json.rdd.map(x => x.mkString("\t")).saveAsTextFile("/user/oathbreaker89/ARUN4/problem7/text-gzip",classOf[org.apache.hadoop.io.compress.GzipCodec])
hadoop fs -ls /user/oathbreaker89/ARUN4/problem7/text-gzip

#8
var order_sequence = sc.sequenceFile("/user/oathbreaker89/ARUN4/problem4/sequence-file",classOf[org.apache.hadoop.io.Text],classOf[org.apache.hadoop.io.Text])

hadoop fs -get /user/oathbreaker89/ARUN4/problem4/sequence-file/part-00000 /home/oathbreaker89/sequence_file

order_sequence.map( x => {var d = x._2.toString.split("\t");(d(0),d(1),d(2),d(3))}).toDF().write.orc("/user/oathbreaker89/ARUN4/problem8/orc")
hadoop fs -ls /user/oathbreaker89/ARUN4/problem8/orc
