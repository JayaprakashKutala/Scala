######################### Sample File Import & Export ########################
spark-shell \
--master yarn \
--packages com.databricks:spark-avro_2.10:2.0.1

import com.databricks.spark.avro._

var ordDF = sqlContext.read.avro("/user/oathbreaker89/Arun1/problem1/orders")

ordDF.show()

var oIDF = sqlContext.read.avro("/user/oathbreaker89/Arun1/problem1/order-items")
oIDF.show()


var joinDF = ordDF.join(oIDF,ordDF("order_id") === oIDF("order_item_order_id"))
joinDF_Filter.show()


joinDF.registerTempTable("table")
var finalDF = sqlContext.sql("SELECT TO_DATE(FROM_UNIXTIME(order_date/1000)) as formatted_date,order_status,count(DISTINCT(order_id)) as total_orders,ROUND(sum(order_item_subtotal),2) as total_amount FROM table GROUP BY TO_DATE(FROM_UNIXTIME(order_date/1000)),order_status ORDER BY formatted_date DESC, order_status,total_amount DESC, total_orders")  


sqlContext.setConf("spark.sql.parquet.compression.codec","Snappy")

finalDF.write.parquet("/user/oathbreaker89/Arun1/problem1/result4a-snappy")

hadoop fs -ls /user/oathbreaker89/Arun1/problem1/result4a-snappy

sqlContext.setConf("spark.sql.parquet.compression.codec", "Gzip")
finalDF.write.parquet("/user/oathbreaker89/Arun1/problem1/result4b-Gzip")

sqlContext.setConf("spark.sql.avro.compression.codec", "Gzip")
finalDF.write.avro("/user/oathbreaker89/Arun1/problem1/result4c-avro-Gzip")


finalDF.write.format("com.databricks.spark.avro").save("/user/oathbreaker89/Arun1/problem1/result4c-avro-Gzip")

finalDF.rdd.map(x => x.mkString(",")).saveAsTextFile("/user/oathbreaker89/Arun1/problem1/result4c-text")
hadoop fs -ls /user/oathbreaker89/Arun1/problem1/result4c-text

finalDF.rdd.map(x => x.mkString(",")).saveAsTextFile("/user/oathbreaker89/Arun1/problem1/result4c-text-gzip",classOf[org.apache.hadoop.io.compress.GzipCodec])
hadoop fs -ls /user/oathbreaker89/Arun1/problem1/result4c-text-gzip

finalDF.toJSON.saveAsTextFile("/user/oathbreaker89/Arun1/problem1/result4d-JSON")
hadoop fs -ls /user/oathbreaker89/Arun1/problem1/result4d-JSON

finalDF.toJSON.saveAsTextFile("/user/oathbreaker89/Arun1/problem1/result4d-JSON-snappy",classOf[org.apache.hadoop.io.compress.SnappyCodec])

oIDF.write.format("com.databricks.spark.avro").save("/user/oathbreaker89/Arun1/problem1/result4c-avro-Gzip")
